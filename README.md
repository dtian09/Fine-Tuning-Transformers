The [Flickr30k](https://huggingface.co/datasets/nlphuji/flickr30k) image dataset is firstly encoded using the CLIP image encoder, then the LlaMA3 8B transformer is fine-tuned on the encoded images plus their captions to predict image captions.  

